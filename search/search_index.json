{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the ATLAS Open Data Hackathon This is a documentation to run the resources to-be-released at the ATLAS Open Data project! Data and MonteCarlo samples at 13 TeV centre-of-mass energy 8 TeV Run 1 samples http://opendata.atlas.cern/extendedanalysis/datasets.php You can get/read one by one, using the URL, from a Jupyter notebook, for example. Or download the ZIP file with all in there. References Online book ATL-OREACH-PUB-2016-001 13 TeV Run 2 samples /eos/project/a/atlas-outreach/projects/open-data/OpenDataTuples/ Please, avoid any modification, you can simply read directly using LXPLUS (use it as read-only area). You need to subscribe to the e-group \u201catlas-outreach-data-and-tools\" to get access. Click here to subscribe Reference ATL-COM-OREACH-2019-002 Notes * We develop the 13 TeV TTree to be compatible with the 8 TeV TTree . * This means that in case something is developed using the 8 TeV samples, can be easily modified to fit the 13 TeV TTree . What are they? They are several collections of samples. The collections are defined by the filters that were applied. Those filters are: 1lep MC (55G) Data (54G) 2lep MC (32G) Data (2.5G) GamGam MC (635M) Data (1.4G) 1lep1tau MC (886M) Data (204M) 1tau Data (191M) MC (107M) 2tau MC (36M) Data (55M) 1fatjet1lep Data (11M) MC (6.0G) More details about the samples, production status can be seen here . Analysis Examples frameworks Explore several of the collections using the Analysis frameworks: Notes DOI and CERN Open Data access Datasets have/will have DOI's that allow their citation.","title":"Home"},{"location":"#welcome-to-the-atlas-open-data-hackathon","text":"This is a documentation to run the resources to-be-released at the ATLAS Open Data project!","title":"Welcome to the ATLAS Open Data Hackathon"},{"location":"#data-and-montecarlo-samples-at-13-tev-centre-of-mass-energy","text":"8 TeV Run 1 samples http://opendata.atlas.cern/extendedanalysis/datasets.php You can get/read one by one, using the URL, from a Jupyter notebook, for example. Or download the ZIP file with all in there. References Online book ATL-OREACH-PUB-2016-001 13 TeV Run 2 samples /eos/project/a/atlas-outreach/projects/open-data/OpenDataTuples/ Please, avoid any modification, you can simply read directly using LXPLUS (use it as read-only area). You need to subscribe to the e-group \u201catlas-outreach-data-and-tools\" to get access. Click here to subscribe Reference ATL-COM-OREACH-2019-002 Notes * We develop the 13 TeV TTree to be compatible with the 8 TeV TTree . * This means that in case something is developed using the 8 TeV samples, can be easily modified to fit the 13 TeV TTree .","title":"Data and MonteCarlo samples at 13 TeV centre-of-mass energy"},{"location":"#what-are-they","text":"They are several collections of samples. The collections are defined by the filters that were applied. Those filters are: 1lep MC (55G) Data (54G) 2lep MC (32G) Data (2.5G) GamGam MC (635M) Data (1.4G) 1lep1tau MC (886M) Data (204M) 1tau Data (191M) MC (107M) 2tau MC (36M) Data (55M) 1fatjet1lep Data (11M) MC (6.0G) More details about the samples, production status can be seen here .","title":"What are they?"},{"location":"#analysis-examples-frameworks","text":"Explore several of the collections using the Analysis frameworks:","title":"Analysis Examples frameworks"},{"location":"#notes","text":"","title":"Notes"},{"location":"#doi-and-cern-open-data-access","text":"Datasets have/will have DOI's that allow their citation.","title":"DOI and CERN Open Data access"},{"location":"datasets/","text":"Analysis codes This is an analysis code that may be used to analyse the data of the ATLAS published dataset. git and gitlab Ideally, you are able to clone a git repository, and you have an GitLab account (if you want to contribute :) C++ Python Virtual Machines (VMs)","title":"Datasets"},{"location":"datasets/#analysis-codes","text":"This is an analysis code that may be used to analyse the data of the ATLAS published dataset.","title":"Analysis codes"},{"location":"datasets/#git-and-gitlab","text":"Ideally, you are able to clone a git repository, and you have an GitLab account (if you want to contribute :)","title":"git and gitlab"},{"location":"datasets/#c","text":"","title":"C++"},{"location":"datasets/#python","text":"","title":"Python"},{"location":"datasets/#virtual-machines-vms","text":"","title":"Virtual Machines (VMs)"},{"location":"document/","text":"Building your project documentation MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown , and configured with a single YAML configuration file. Mkdocs installation Prerequisite: Python and Pip In order to manually install MkDocs you'll need Python installed on your system, as well as the Python package manager, pip. You can check if you have these already installed from the command line: On Windows: C:\\Users\\User>python --version Python 3.5.4 C:\\Users\\User>pip --version pip 9.0.1 On MacOS or Linux $ python --version Python 2.7.2 $ pip --version pip 1.5.2 Installing Python Install Python by downloading an installer appropriate for your system from python.org and running it. Note: If you are installing Python on Windows, be sure to check the box to have Python added to your PATH if the installer offers such an option (it's normally off by default). Installing Pip If you're using a recent version of Python, the Python package manager, pip, is most likely installed by default. However, you may need to upgrade pip to the lasted version: pip install --upgrade pip If you need to install pip for the first time, download get-pip.py. Then run the following command to install it: python get-pip.py Installing MkDocs Install the mkdocs package using pip: pip install mkdocs You should now have the mkdocs command installed on your system. Run mkdocs--version to check whether successfully installed. On Windows: C:\\Users\\User>mkdocs --version mkdocs, version 1.0.4 On MacOS or Linux $ mkdocs --version mkdocs, version 0.15.3 Getting started to edit Clone this documentation to your device by git Clone with ssh: git clone ssh://git@gitlab.cern.ch:7999/atlas-outreach-data-tools/hackathon-docs.git Clone with https: git clone https://gitlab.cern.ch/atlas-outreach-data-tools/hackathon-docs.git In your hackathon-docs folder, you will see a configuration file named mkdocs.yml , and a folder named docs that will contain your documentation source files. Right now the docs folder just contains all the documentation pages, such as index.md , datasets.md and the folders where the pages are saved. MkDocs comes with a built-in dev-server that lets you preview your documentation as you work on it. Make sure you're in the same directory as the mkdocs.yml configuration file, and then start the server by running the mkdocs serve command: C:\\Users\\User\\hackathon\\hackathon-docs>mkdocs serve INFO - Building documentation... INFO - Cleaning site directory [I 190705 15:13:53 server:296] Serving on http://127.0.0.1:8000 [I 190705 15:13:53 handlers:62] Start watching changes [I 190705 15:13:53 handlers:64] Start detecting changes Open up http://127.0.0.1:8000/ in your browser, and you'll see the pages you cloned being displayed: Use the editor you like to edit the files in docs and your changes will be displayed when saving them. Using the Git to commit your changes By far, the most widely used modern version control system in the world today is Git. In Git, every developer's working copy of the code is also a repository that can contain the full history of all changes. Prerequisites: Download and install Git Apply for a Gitlab account The working flow of Git: Initialize your repository with git init[directory] Clone a repository onto your local machine with git clone [URL] Edit the docs using Mkdocs Stage all changes for the next conmmit with git add [directory] Commit the staged snapshot with git commit -m [\"commit message\"] If others have modified it, you can check the updates with git fetch and update the resource with git pull . Show unstaged changes between your index and working directory with git diff Review your changes before submitting with git status Push your changes to the remote Gitlab repository with git push Reference to git cheet sheet for details","title":"How to contribute to this book"},{"location":"document/#building-your-project-documentation","text":"MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown , and configured with a single YAML configuration file.","title":"Building your project documentation"},{"location":"document/#mkdocs-installation","text":"Prerequisite: Python and Pip In order to manually install MkDocs you'll need Python installed on your system, as well as the Python package manager, pip. You can check if you have these already installed from the command line: On Windows: C:\\Users\\User>python --version Python 3.5.4 C:\\Users\\User>pip --version pip 9.0.1 On MacOS or Linux $ python --version Python 2.7.2 $ pip --version pip 1.5.2 Installing Python Install Python by downloading an installer appropriate for your system from python.org and running it. Note: If you are installing Python on Windows, be sure to check the box to have Python added to your PATH if the installer offers such an option (it's normally off by default). Installing Pip If you're using a recent version of Python, the Python package manager, pip, is most likely installed by default. However, you may need to upgrade pip to the lasted version: pip install --upgrade pip If you need to install pip for the first time, download get-pip.py. Then run the following command to install it: python get-pip.py Installing MkDocs Install the mkdocs package using pip: pip install mkdocs You should now have the mkdocs command installed on your system. Run mkdocs--version to check whether successfully installed. On Windows: C:\\Users\\User>mkdocs --version mkdocs, version 1.0.4 On MacOS or Linux $ mkdocs --version mkdocs, version 0.15.3","title":"Mkdocs installation"},{"location":"document/#getting-started-to-edit","text":"Clone this documentation to your device by git Clone with ssh: git clone ssh://git@gitlab.cern.ch:7999/atlas-outreach-data-tools/hackathon-docs.git Clone with https: git clone https://gitlab.cern.ch/atlas-outreach-data-tools/hackathon-docs.git In your hackathon-docs folder, you will see a configuration file named mkdocs.yml , and a folder named docs that will contain your documentation source files. Right now the docs folder just contains all the documentation pages, such as index.md , datasets.md and the folders where the pages are saved. MkDocs comes with a built-in dev-server that lets you preview your documentation as you work on it. Make sure you're in the same directory as the mkdocs.yml configuration file, and then start the server by running the mkdocs serve command: C:\\Users\\User\\hackathon\\hackathon-docs>mkdocs serve INFO - Building documentation... INFO - Cleaning site directory [I 190705 15:13:53 server:296] Serving on http://127.0.0.1:8000 [I 190705 15:13:53 handlers:62] Start watching changes [I 190705 15:13:53 handlers:64] Start detecting changes Open up http://127.0.0.1:8000/ in your browser, and you'll see the pages you cloned being displayed: Use the editor you like to edit the files in docs and your changes will be displayed when saving them.","title":"Getting started to edit"},{"location":"document/#using-the-git-to-commit-your-changes","text":"By far, the most widely used modern version control system in the world today is Git. In Git, every developer's working copy of the code is also a repository that can contain the full history of all changes. Prerequisites: Download and install Git Apply for a Gitlab account The working flow of Git: Initialize your repository with git init[directory] Clone a repository onto your local machine with git clone [URL] Edit the docs using Mkdocs Stage all changes for the next conmmit with git add [directory] Commit the staged snapshot with git commit -m [\"commit message\"] If others have modified it, you can check the updates with git fetch and update the resource with git pull . Show unstaged changes between your index and working directory with git diff Review your changes before submitting with git status Push your changes to the remote Gitlab repository with git push Reference to git cheet sheet for details","title":"Using the Git to commit your changes"},{"location":"C++/cpp/","text":"ATLAS Open Data C++ framework for 13 TeV analyses GitLab repository atlas-outreach-cpp-framework-13tev git clone https://gitlab.cern.ch/lserkin/atlas-outreach-cpp-framework-13tev.git About This is the C++ analysis code that may be used to analyse the data of the ATLAS published dataset. More information in the 8 TeV ATLAS Open Data PUB notes: https://cds.cern.ch/record/2203649/files/ATL-OREACH-PUB-2016-001.pdf https://cds.cern.ch/record/2624572/files/ATL-OREACH-PUB-2018-001.pdf This code is made of two parts of codes: analysis codes and plotting codes. Setup After checking out the repository, you need to setup ROOT framework and gcc compiler. The current version was compiled using gcc v6.20 and root v6.10.04 Samples For now, the analysis code can be run using samples located in eos: /eos/project/a/atlas-outreach/projects/open-data/OpenDataTuples/ Analysis Code The analysis code is located in the Analysis folder. It will be used to write out histograms for the individual input files which will be used for plotting purposes later. There are currently 7 available analyses stored as \" NNAnalysis \", where NN = WBoson, ZBoson, TTbar, WZDiBoson, ZZDiBoson, HWW, ZPrime Each analysis code contains - the analysis header ( NNAnalysis.h ) which defines the histograms and gives access to the variables stored in the input samples - the histogram header ( NNAnalysisHistograms.h ) which defines the name of output histograms - the analysis code ( NNAnalysis.C ) which makes all the selection and stores the output histograms - the analysis main ( main_NNAnalysis.C ) controls which samples are being used - the output directory ( Output_NNAnalysis ) is where the output histograms (one per each input sample) will be stored First setup the code using: * ./welcome.sh or * source welcome.sh To run the code: * ./run.sh or * source run.sh Shared libraries will be created and the analysis will run over each input sample. Estimated time it takes to run an analysis code: 15 seconds per 1M events To clean all shared libraries: * ./clean.sh or * source clean.sh Plotting The plotting code is located in the Plotting folder. * Compile the plotting code with the command: make Run the code with: ./plot [WBosonAnalysis, ZBosonAnalysis, TTbarAnalysis, DiBosonAnalysis, HWWAnalysis, ZPrimeAnalysis] [location of OutputDir_AnalysisName] Here you have to choose * 1.) which analysis you will run * 2.) where is the output of the analysis code you produced Examples ./plot ZBosonAnalysis ../Analysis/ZBosonAnalysis/OutputDir_ZBosonAnalysis The plots are saved inside the directory \" histograms \", don't forget to rename in case you run over several analyses Which histograms are being produced? This is controlled by the input file \" HistoList_ANALYSISNAME.txt \" inside list_histos Do you want to add a new plot? just add the name of it in this file (of course it has to exist in the output from the analysis code) In case you changed the Plotting code, clean first: make clean; make Auxiliary: Files.txt contains the name, cross-section, sum of weights and efficiency for each of the MC samples. DO NOT CHANGE How to add a new variable and plot it Add in the header ( NNAnalysis.h ) the new histogram: TH1F *h_new = 0; Add in the main analysis code ( NNAnalysis.C ) the calculation of the new variable: float new_variable = jet_n; Add in NNAnalysisHistograms.h in define_histograms(): h_new = new TH1F(\"h_new\", \"Description of the new variable; X axis name ; Y axis name \", number of bins , min bin , max bin); in FillOutputList() GetOutputList()->Add(h_new); in WriteHistograms() h_new->Write(); in FillHistograms() if (s.Contains(\"h_new\")) h_new->Fill(m,w); Now finally add in NNAnalysis.C the connection between the new variable and the new histogram FillHistograms( new_variable, weight, \"h_new\"); where \"new_variable\" is the new variable you calculated in 2. now run the code again over all the samples Go to PlottingCode/list_histos in HistoList_NNAnalysis.txt add one new line: h_new run the plotting code and you will find the new histogram in histograms/h_new.png !","title":"C++ framework"},{"location":"C++/cpp/#atlas-open-data-c-framework-for-13-tev-analyses","text":"","title":"ATLAS Open Data C++ framework for 13 TeV analyses"},{"location":"C++/cpp/#gitlab-repository","text":"atlas-outreach-cpp-framework-13tev git clone https://gitlab.cern.ch/lserkin/atlas-outreach-cpp-framework-13tev.git","title":"GitLab repository"},{"location":"C++/cpp/#about","text":"This is the C++ analysis code that may be used to analyse the data of the ATLAS published dataset. More information in the 8 TeV ATLAS Open Data PUB notes: https://cds.cern.ch/record/2203649/files/ATL-OREACH-PUB-2016-001.pdf https://cds.cern.ch/record/2624572/files/ATL-OREACH-PUB-2018-001.pdf This code is made of two parts of codes: analysis codes and plotting codes.","title":"About"},{"location":"C++/cpp/#setup","text":"After checking out the repository, you need to setup ROOT framework and gcc compiler. The current version was compiled using gcc v6.20 and root v6.10.04","title":"Setup"},{"location":"C++/cpp/#samples","text":"For now, the analysis code can be run using samples located in eos: /eos/project/a/atlas-outreach/projects/open-data/OpenDataTuples/","title":"Samples"},{"location":"C++/cpp/#analysis-code","text":"The analysis code is located in the Analysis folder. It will be used to write out histograms for the individual input files which will be used for plotting purposes later. There are currently 7 available analyses stored as \" NNAnalysis \", where NN = WBoson, ZBoson, TTbar, WZDiBoson, ZZDiBoson, HWW, ZPrime Each analysis code contains - the analysis header ( NNAnalysis.h ) which defines the histograms and gives access to the variables stored in the input samples - the histogram header ( NNAnalysisHistograms.h ) which defines the name of output histograms - the analysis code ( NNAnalysis.C ) which makes all the selection and stores the output histograms - the analysis main ( main_NNAnalysis.C ) controls which samples are being used - the output directory ( Output_NNAnalysis ) is where the output histograms (one per each input sample) will be stored First setup the code using: * ./welcome.sh or * source welcome.sh To run the code: * ./run.sh or * source run.sh Shared libraries will be created and the analysis will run over each input sample. Estimated time it takes to run an analysis code: 15 seconds per 1M events To clean all shared libraries: * ./clean.sh or * source clean.sh","title":"Analysis Code"},{"location":"C++/cpp/#plotting","text":"The plotting code is located in the Plotting folder. * Compile the plotting code with the command: make Run the code with: ./plot [WBosonAnalysis, ZBosonAnalysis, TTbarAnalysis, DiBosonAnalysis, HWWAnalysis, ZPrimeAnalysis] [location of OutputDir_AnalysisName] Here you have to choose * 1.) which analysis you will run * 2.) where is the output of the analysis code you produced Examples ./plot ZBosonAnalysis ../Analysis/ZBosonAnalysis/OutputDir_ZBosonAnalysis The plots are saved inside the directory \" histograms \", don't forget to rename in case you run over several analyses Which histograms are being produced? This is controlled by the input file \" HistoList_ANALYSISNAME.txt \" inside list_histos Do you want to add a new plot? just add the name of it in this file (of course it has to exist in the output from the analysis code) In case you changed the Plotting code, clean first: make clean; make Auxiliary: Files.txt contains the name, cross-section, sum of weights and efficiency for each of the MC samples. DO NOT CHANGE","title":"Plotting"},{"location":"C++/cpp/#how-to-add-a-new-variable-and-plot-it","text":"Add in the header ( NNAnalysis.h ) the new histogram: TH1F *h_new = 0; Add in the main analysis code ( NNAnalysis.C ) the calculation of the new variable: float new_variable = jet_n; Add in NNAnalysisHistograms.h in define_histograms(): h_new = new TH1F(\"h_new\", \"Description of the new variable; X axis name ; Y axis name \", number of bins , min bin , max bin); in FillOutputList() GetOutputList()->Add(h_new); in WriteHistograms() h_new->Write(); in FillHistograms() if (s.Contains(\"h_new\")) h_new->Fill(m,w); Now finally add in NNAnalysis.C the connection between the new variable and the new histogram FillHistograms( new_variable, weight, \"h_new\"); where \"new_variable\" is the new variable you calculated in 2. now run the code again over all the samples Go to PlottingCode/list_histos in HistoList_NNAnalysis.txt add one new line: h_new run the plotting code and you will find the new histogram in histograms/h_new.png !","title":"How to add a new variable and plot it"},{"location":"C++/times/","text":"ATLAS Open Data C++ framework for 13 TeV analyses About Z boson (ZBosonAnalysis) 993.55s user 20.04s system 90% cpu 18:44.92 total H->gg (HyyAnalysis) 84.08s user 5.78s system 95% cpu 1:33.77 total H->ZZ ./run.sh 658.76s user 17.76s system 85% cpu 13:12.13 total (ZZDiBosonAnalysis) WZ ./run.sh 678.57s user 17.11s system 63% cpu 18:20.84 total (WZDiBosonAnalysis) TTbar ./run.sh 2770.97s user 58.98s system 88% cpu 53:03.66 total (TTbarAnalysis) W boson ./run.sh 3277.89s user 46.79s system 91% cpu 1:00:28.64 total (WBosonAnalysis) Note Version of ROOT >6.14 give problem with the include \"TProof.h\"","title":"C++ notes"},{"location":"C++/times/#atlas-open-data-c-framework-for-13-tev-analyses","text":"","title":"ATLAS Open Data C++ framework for 13 TeV analyses"},{"location":"C++/times/#about","text":"Z boson (ZBosonAnalysis) 993.55s user 20.04s system 90% cpu 18:44.92 total H->gg (HyyAnalysis) 84.08s user 5.78s system 95% cpu 1:33.77 total H->ZZ ./run.sh 658.76s user 17.76s system 85% cpu 13:12.13 total (ZZDiBosonAnalysis) WZ ./run.sh 678.57s user 17.11s system 63% cpu 18:20.84 total (WZDiBosonAnalysis) TTbar ./run.sh 2770.97s user 58.98s system 88% cpu 53:03.66 total (TTbarAnalysis) W boson ./run.sh 3277.89s user 46.79s system 91% cpu 1:00:28.64 total (WBosonAnalysis) Note Version of ROOT >6.14 give problem with the include \"TProof.h\"","title":"About"},{"location":"Python/python/","text":"ATLAS Open Data Python framework for 13 TeV analyses About This is an analysis code that may be used to analyse the data of the ATLAS published dataset. GitLab repository ATLASDatasetTools13 git clone https://gitlab.cern.ch/meevans/ATLASDatasetTools13.git General Usage Analysis The files in the root directory of the installation are the various run scripts. Configuration files can be found in the Configurations folder. As a first test to check whether everything works fine you can simply run a preconfigured analyis via python RunScript.py -s \"Zmumu\" What you have done here is to run the code in single core mode and specifying that you only want to analyse the Zmumu sample as defined in the Configurations/2lepConfiguration.py. The runscript has several options which may be displayed by typing python RunScript.py --help The options include: -a, --analysis overrides the analysis that is stated in the configuration file -s, --samples comma separated string that contains the keys for a subset of processes to run over -p, --parallel enables running in parallel (default is single core use) -n NWORKERS, --nWorkers NWORKERS specifies the number of workers if multi core usage is desired (default is 4) -c CONFIGFILE, --configfile CONFIGFILE specifies the config file to be read (default is Configurations/Configuration.py) -o OUTPUTDIR, --output OUTPUDIR specifies the output directory you would like to use instead of the one in the configuration file The XConfiguration.py files specify how an analysis should behave. The Job portion of the configuration looks like this: Job = { \"Batch\" : True, (switches progress bar on and off, forced to be off when running in parallel mode) \"Analysis\" : \"ZAnalysis\", (names the analysis to be executed) \"Fraction\" : 0.000001, (determines the fraction of events per file to be analysed) \"MaxEvents\" : 1234567890, (determines the maximum number of events per file to be analysed) \"OutputDirectory\" : \"results/\" (specifies the directory where the output root files should be saved) } The second portion of the configuration file specifies which The locations of the individual files that are to be used for the different processes can be set es such: Processes = { # Diboson processes 2lep > \"WWlvlv\" : \"/eos/project/a/atlas-outreach/projects/open-data/OpenDataTuples/2lep/MC/mc15_13TeV.361600.PwPy8EG_CT10nloME_AZNLOCTEQ6L1_WWlvlv.2lep_raw.root\", (single file) ... \"data_2lep\" : \"/eos/project/a/atlas-outreach/projects/open-data/OpenDataTuples/2lep/Data/data*_2lep.root\", (potentially many files) } The files associated with the processes are found via python's glob module, enabling the use of unix style wildcards. The names chosen for the processes are important as they are the keys that are used later in the infofile.py to determine the necessary scaling factors for correct plotting. Now we want to run over the full set of available samples. For this simply type: python RunScript.py Use the options -p and -n if you have a multi core system and want to use multiple cores. Execution times are between ? to ? hours in single core mode or ~ ? minutes in multi core mode. Plotting Results may be plotted via: python PlotResults.py Configuration/PlotConf_AnalysisName.py In our example case the name of the analysis is ZAnalysis , so type: python PlotResults.py Configuration/PlotConf_ZAnalysis.py The resulting histograms will be put into the Output directory. The plotting configuration file enables the user to steer the plotting process. Each analysis has its own plotting configuration file to accomodate changes in background composition or histograms that the user may want to plot. General information for plotting include the Luminosity and InputDirectory located at the top of the file: config = { \"Luminosity\" : 10064, \"InputDirectory\" : \"results\", ... The names of the histograms to be drawn can be specified like so: \"Histograms\" : { \"etmiss\" : {rebin : 4, log_y : True}, \"lep_n\" : {rebin : 5}, \"lep_pt\" : {}, ... Note that it is possible to supply additional information via a dictionary like structure to further detail the per histogram options. Currently available options are: rebin : int - used to merge X bins into one. Useful in low statistics situations log_y : bool - if True is set as the bool the main depiction will be drawn in logarithmic scale y_margin : float - sets the fraction of whitespace above the largest contribution in the plot. Default value is 0.1. Definition of Paintables and Depictions Each Plot consists of several depictions of paintables . A depiction is a certain standard type of visualising information. Availabe depictions include simple plots, ratios and agreement plots. A paintable is a histogram or stack with added information such as colors and which processes contribute to said histogram. A simple definition of paintables may look like this: 'Paintables': { \"Stack\": { \"Order\" : [\"Diboson\", \"W\", \"Z\", \"single top\", \"ttbar\"], \"Processes\" : { \"Diboson\" : { \"Color\" : \"#fa7921\", \"Contributions\" : [\"WWlvlv\", \"WWlvqq\", \"WZlvll\", \"WZlvvv\", \"WZqqll\", \"WZlvqq\", \"ZZllll\", \"ZZvvll\", \"ZZqqll\"]}, ... }, 'Higgs': { 'Color': '#0000ff', 'Contributions': ['ggH125_WW2lep']}, \"data\" : { \"Contributions\": [\"dataA_2lep\", \"dataB_2lep\", \"dataC_2lep\", \"dataD_2lep\"]} Stack and data are specialised names for paintables . This ensures that only one stack and one data representation are present in the visual results. A Stack shows the different processes specified in \"order\" stacked upon each other to give an idea of the composition of the simulated data. The definitions for these individual processes are defined under \"Processes\". Each process has a certain colour and a list of contributing parts that comprise it. These contributing parts have to fit the keys used in both the run configuration and the infofile.py . data is a specialised paintable which is geared toward the standard representation of data. Since the data does not need to be scaled there is no need to align the used names in contributions with those found in the infofile.py . However, they still have to fit the ones used in the configuration.py . All otherwise named paintables (like \"Higgs\" in the example) are considered as \"overlays\". Overlays are used to show possible signals or to compare shapes between multiple overlays (see for instance in the HWWAnalysis). The paintables can be used in depictions like so: \"Depictions\": { \"Order\" : [\"Main\", \"Data/MC\", \"S/B\"], \"Definitions\" : { \"Data/MC\": { \"type\" : \"Agreement\", \"Paintables\" : [\"data\", \"Stack\"]}, \"Main\": { \"type\" : \"Main\", \"Paintables\": [\"Stack\", \"data\"]}, 'S/B': { 'type' : 'Ratio', 'Paintables' : ['Higgs', 'Stack']}, } There are currently three types of depictions available: Main , Agreement and Ratio . Main type plots will simply show the paintables in a simple plot fashion. Ratio type plots will show the ratio of the first paintable w.r.t. the second paintable. Agreement type plots are typically used to evaluate the agreement between two paintables (usually the stack of predictions and the data). The order of the depictions is determined in line 2 of the code example above. In Depth Information Analysis Code The analysis code is located in the Analysis folder. It will be used to write out histograms for the individual input files which will be used for plotting purposes later. The basic code implementing the protocol to read the files and how the objects can be read is in Tuplereader.py . Have a look there to see which information is available. The general analysis flow can be found in Job.py whereas the base class for all concrete analyses is located in Analysis.py . It is recommended to start out by modifying one of the existing analyses, e.g. the ZAnalysis located in ZAnalysis.py . If you want to add an analysis, make sure that the filename is the same as the class name, otherwise the code will not work.","title":"Python framework"},{"location":"Python/python/#atlas-open-data-python-framework-for-13-tev-analyses","text":"","title":"ATLAS Open Data Python framework for 13 TeV analyses"},{"location":"Python/python/#about","text":"This is an analysis code that may be used to analyse the data of the ATLAS published dataset.","title":"About"},{"location":"Python/python/#gitlab-repository","text":"ATLASDatasetTools13 git clone https://gitlab.cern.ch/meevans/ATLASDatasetTools13.git","title":"GitLab repository"},{"location":"Python/python/#general-usage","text":"","title":"General Usage"},{"location":"Python/python/#analysis","text":"The files in the root directory of the installation are the various run scripts. Configuration files can be found in the Configurations folder. As a first test to check whether everything works fine you can simply run a preconfigured analyis via python RunScript.py -s \"Zmumu\" What you have done here is to run the code in single core mode and specifying that you only want to analyse the Zmumu sample as defined in the Configurations/2lepConfiguration.py. The runscript has several options which may be displayed by typing python RunScript.py --help The options include: -a, --analysis overrides the analysis that is stated in the configuration file -s, --samples comma separated string that contains the keys for a subset of processes to run over -p, --parallel enables running in parallel (default is single core use) -n NWORKERS, --nWorkers NWORKERS specifies the number of workers if multi core usage is desired (default is 4) -c CONFIGFILE, --configfile CONFIGFILE specifies the config file to be read (default is Configurations/Configuration.py) -o OUTPUTDIR, --output OUTPUDIR specifies the output directory you would like to use instead of the one in the configuration file The XConfiguration.py files specify how an analysis should behave. The Job portion of the configuration looks like this: Job = { \"Batch\" : True, (switches progress bar on and off, forced to be off when running in parallel mode) \"Analysis\" : \"ZAnalysis\", (names the analysis to be executed) \"Fraction\" : 0.000001, (determines the fraction of events per file to be analysed) \"MaxEvents\" : 1234567890, (determines the maximum number of events per file to be analysed) \"OutputDirectory\" : \"results/\" (specifies the directory where the output root files should be saved) } The second portion of the configuration file specifies which The locations of the individual files that are to be used for the different processes can be set es such: Processes = { # Diboson processes 2lep > \"WWlvlv\" : \"/eos/project/a/atlas-outreach/projects/open-data/OpenDataTuples/2lep/MC/mc15_13TeV.361600.PwPy8EG_CT10nloME_AZNLOCTEQ6L1_WWlvlv.2lep_raw.root\", (single file) ... \"data_2lep\" : \"/eos/project/a/atlas-outreach/projects/open-data/OpenDataTuples/2lep/Data/data*_2lep.root\", (potentially many files) } The files associated with the processes are found via python's glob module, enabling the use of unix style wildcards. The names chosen for the processes are important as they are the keys that are used later in the infofile.py to determine the necessary scaling factors for correct plotting. Now we want to run over the full set of available samples. For this simply type: python RunScript.py Use the options -p and -n if you have a multi core system and want to use multiple cores. Execution times are between ? to ? hours in single core mode or ~ ? minutes in multi core mode.","title":"Analysis"},{"location":"Python/python/#plotting","text":"Results may be plotted via: python PlotResults.py Configuration/PlotConf_AnalysisName.py In our example case the name of the analysis is ZAnalysis , so type: python PlotResults.py Configuration/PlotConf_ZAnalysis.py The resulting histograms will be put into the Output directory. The plotting configuration file enables the user to steer the plotting process. Each analysis has its own plotting configuration file to accomodate changes in background composition or histograms that the user may want to plot. General information for plotting include the Luminosity and InputDirectory located at the top of the file: config = { \"Luminosity\" : 10064, \"InputDirectory\" : \"results\", ... The names of the histograms to be drawn can be specified like so: \"Histograms\" : { \"etmiss\" : {rebin : 4, log_y : True}, \"lep_n\" : {rebin : 5}, \"lep_pt\" : {}, ... Note that it is possible to supply additional information via a dictionary like structure to further detail the per histogram options. Currently available options are: rebin : int - used to merge X bins into one. Useful in low statistics situations log_y : bool - if True is set as the bool the main depiction will be drawn in logarithmic scale y_margin : float - sets the fraction of whitespace above the largest contribution in the plot. Default value is 0.1.","title":"Plotting"},{"location":"Python/python/#definition-of-paintables-and-depictions","text":"Each Plot consists of several depictions of paintables . A depiction is a certain standard type of visualising information. Availabe depictions include simple plots, ratios and agreement plots. A paintable is a histogram or stack with added information such as colors and which processes contribute to said histogram. A simple definition of paintables may look like this: 'Paintables': { \"Stack\": { \"Order\" : [\"Diboson\", \"W\", \"Z\", \"single top\", \"ttbar\"], \"Processes\" : { \"Diboson\" : { \"Color\" : \"#fa7921\", \"Contributions\" : [\"WWlvlv\", \"WWlvqq\", \"WZlvll\", \"WZlvvv\", \"WZqqll\", \"WZlvqq\", \"ZZllll\", \"ZZvvll\", \"ZZqqll\"]}, ... }, 'Higgs': { 'Color': '#0000ff', 'Contributions': ['ggH125_WW2lep']}, \"data\" : { \"Contributions\": [\"dataA_2lep\", \"dataB_2lep\", \"dataC_2lep\", \"dataD_2lep\"]} Stack and data are specialised names for paintables . This ensures that only one stack and one data representation are present in the visual results. A Stack shows the different processes specified in \"order\" stacked upon each other to give an idea of the composition of the simulated data. The definitions for these individual processes are defined under \"Processes\". Each process has a certain colour and a list of contributing parts that comprise it. These contributing parts have to fit the keys used in both the run configuration and the infofile.py . data is a specialised paintable which is geared toward the standard representation of data. Since the data does not need to be scaled there is no need to align the used names in contributions with those found in the infofile.py . However, they still have to fit the ones used in the configuration.py . All otherwise named paintables (like \"Higgs\" in the example) are considered as \"overlays\". Overlays are used to show possible signals or to compare shapes between multiple overlays (see for instance in the HWWAnalysis). The paintables can be used in depictions like so: \"Depictions\": { \"Order\" : [\"Main\", \"Data/MC\", \"S/B\"], \"Definitions\" : { \"Data/MC\": { \"type\" : \"Agreement\", \"Paintables\" : [\"data\", \"Stack\"]}, \"Main\": { \"type\" : \"Main\", \"Paintables\": [\"Stack\", \"data\"]}, 'S/B': { 'type' : 'Ratio', 'Paintables' : ['Higgs', 'Stack']}, } There are currently three types of depictions available: Main , Agreement and Ratio . Main type plots will simply show the paintables in a simple plot fashion. Ratio type plots will show the ratio of the first paintable w.r.t. the second paintable. Agreement type plots are typically used to evaluate the agreement between two paintables (usually the stack of predictions and the data). The order of the depictions is determined in line 2 of the code example above.","title":"Definition of Paintables and Depictions"},{"location":"Python/python/#in-depth-information","text":"","title":"In Depth Information"},{"location":"Python/python/#analysis-code","text":"The analysis code is located in the Analysis folder. It will be used to write out histograms for the individual input files which will be used for plotting purposes later. The basic code implementing the protocol to read the files and how the objects can be read is in Tuplereader.py . Have a look there to see which information is available. The general analysis flow can be found in Job.py whereas the base class for all concrete analyses is located in Analysis.py . It is recommended to start out by modifying one of the existing analyses, e.g. the ZAnalysis located in ZAnalysis.py . If you want to add an analysis, make sure that the filename is the same as the class name, otherwise the code will not work.","title":"Analysis Code"},{"location":"vm/","text":"About This is an analysis code that may be used to analyse the data of the ATLAS published dataset. Why do you need a virtual machine? Virtualization allows you to create an operating system in the operating system and test programs without installing them on the host machine. Also virtualization allows you to do penetration testing. There are a generous amount of Virtual Machines (VM): ORACLE VirtualBox , Windows Visual , VMware Workstation , Colinux , AlphaVM-Pro , DOSBox , Linux-VServer , PearPC , VirtualLogix VLX etc. (read more about VM on wikipedia ). Among these VM, Oracle VirtualBox is one of the best. VirtualBox is a virtualization software product for Microsoft Windows, Linux, FreeBSD, Mac OS X, Solaris / OpenSolaris, ReactOS, DOS and others. Collaboration Software Products - Top To Look For: \u2014 USB support - VirtualBox implements a virtual USB controller and supports connecting devices to a virtual machine via USB. \u2014 Built-in RDP server, as well as support for USB client devices over RDP \u2014 the user can connect to the virtual machine remotely using an RDP-compatible client. \u2014 iSCSI initiator - VirtualBox contains an integral iSCSI initiator that makes it possible to use an iSCSI worker as a virtual disk without the need to support the iSCSI guest system. \u2014 Support for various types of network interaction (NAT, Host Networking via Bridged, Internal). \u2014 Support for Shared Folders, allowing you to share files between the host and guest systems. \u2014 and finally VirtualBox is a free to use. Also with VirtualBox, you can use Windows, Linux, Mac OS X, Solaris to run their bundles on one computer. Each virtual machine is called a guest, and your main computer (the OS installed on it) is called a host. The point is that you can run several guest systems in the host\u2019s operating system (direct inside it). VM with ROOT and Jupyter VirtualBox installation Follow the instructions below to install the VM on your computer: - What version to download ? Assumption of what your operating system. In this review we will explain how to install VirtualBox on Windows 10. Installing VirtualBox on all versions of Windows is identical. First download the installation file. \u2014 Download VirtualBox you can free on the official website . \u2014 So, after you have downloaded the latest version of Virtual Box, let's proceed to the installation. Find the downloaded file in your computer (usually the downloaded file is in \u0421:\\Users\\User\\Downloads\\\"filename\" ), run the program and click \"Next\". \u2014 After that, the component selection window will appear. Without changing anything, click \"Next\". \u2014 In the next window, without any changing click \"Next\" \u2014 Now a window will appear that says that the Internet will be temporarily disabled during the installation of the program. Click \"Yes\". \u2014 And click \"Install\" to begin the installation. \u2014 After the installation process is complete, click \"Finish\" \u2014 Now you will see a clean window of your virtual machine without any operating systems: Configure VirtualBox \u2014 If you have English installed on your system, the program will automatically change the interface language to English when you first start it. If this does not happen, go to the menu \u201cFile\u201d -> \u201cSettings\\or/Preferences\u201d and on the Languages tab select your language . In principle, in the settings of VirtualBox there is nothing more to change. How to install an operating system in your virtual machine? First, download the ready operating system (.ova file with ROOT and Jupiter inside) from this link ! If you want to install your operating system from scratch, then see how to do it in this section . \u2014 After you have downloaded the ready file, start the VirtualBox. Go to the menu \"File\" -> \u201cImport Appliance...\u201d (or perform the same function with the combination of buttons: \"Ctrl\" + \"I\" ). \u2014 In the window that appears, select the downloaded \".ova\" file from the Downloads folder and click \"Next\": \u2014 After click \"Import\" without any changes: \u2014 The operating system is importing: \u2014 Now your Virtual Machine with Linux operating system is ready to work, start it by clicking and use: Jupiter terminal start and Notebooks run errors in VM After starting your VM and running Notebooks, you may encounter the following error (look at the red frames in the photo below): To solve this error , follow these simple steps: \u2014 \u0421lose notebooks and terminal \u2014 Run terminal again and type this code ./run-server-jupyter.sh to start Jupiter \u2014 You have restarted your terminal and server successfully \u2014 Open notebook in your browser and restart kernel by clicking restart button Now your terminal and server should work. Notebook also runs without errors.","title":"Virtual Machines"},{"location":"vm/#about","text":"This is an analysis code that may be used to analyse the data of the ATLAS published dataset. Why do you need a virtual machine? Virtualization allows you to create an operating system in the operating system and test programs without installing them on the host machine. Also virtualization allows you to do penetration testing. There are a generous amount of Virtual Machines (VM): ORACLE VirtualBox , Windows Visual , VMware Workstation , Colinux , AlphaVM-Pro , DOSBox , Linux-VServer , PearPC , VirtualLogix VLX etc. (read more about VM on wikipedia ). Among these VM, Oracle VirtualBox is one of the best. VirtualBox is a virtualization software product for Microsoft Windows, Linux, FreeBSD, Mac OS X, Solaris / OpenSolaris, ReactOS, DOS and others. Collaboration Software Products - Top To Look For: \u2014 USB support - VirtualBox implements a virtual USB controller and supports connecting devices to a virtual machine via USB. \u2014 Built-in RDP server, as well as support for USB client devices over RDP \u2014 the user can connect to the virtual machine remotely using an RDP-compatible client. \u2014 iSCSI initiator - VirtualBox contains an integral iSCSI initiator that makes it possible to use an iSCSI worker as a virtual disk without the need to support the iSCSI guest system. \u2014 Support for various types of network interaction (NAT, Host Networking via Bridged, Internal). \u2014 Support for Shared Folders, allowing you to share files between the host and guest systems. \u2014 and finally VirtualBox is a free to use. Also with VirtualBox, you can use Windows, Linux, Mac OS X, Solaris to run their bundles on one computer. Each virtual machine is called a guest, and your main computer (the OS installed on it) is called a host. The point is that you can run several guest systems in the host\u2019s operating system (direct inside it).","title":"About"},{"location":"vm/#vm-with-root-and-jupyter","text":"VirtualBox installation Follow the instructions below to install the VM on your computer: - What version to download ? Assumption of what your operating system. In this review we will explain how to install VirtualBox on Windows 10. Installing VirtualBox on all versions of Windows is identical. First download the installation file. \u2014 Download VirtualBox you can free on the official website . \u2014 So, after you have downloaded the latest version of Virtual Box, let's proceed to the installation. Find the downloaded file in your computer (usually the downloaded file is in \u0421:\\Users\\User\\Downloads\\\"filename\" ), run the program and click \"Next\". \u2014 After that, the component selection window will appear. Without changing anything, click \"Next\". \u2014 In the next window, without any changing click \"Next\" \u2014 Now a window will appear that says that the Internet will be temporarily disabled during the installation of the program. Click \"Yes\". \u2014 And click \"Install\" to begin the installation. \u2014 After the installation process is complete, click \"Finish\" \u2014 Now you will see a clean window of your virtual machine without any operating systems: Configure VirtualBox \u2014 If you have English installed on your system, the program will automatically change the interface language to English when you first start it. If this does not happen, go to the menu \u201cFile\u201d -> \u201cSettings\\or/Preferences\u201d and on the Languages tab select your language . In principle, in the settings of VirtualBox there is nothing more to change. How to install an operating system in your virtual machine? First, download the ready operating system (.ova file with ROOT and Jupiter inside) from this link ! If you want to install your operating system from scratch, then see how to do it in this section . \u2014 After you have downloaded the ready file, start the VirtualBox. Go to the menu \"File\" -> \u201cImport Appliance...\u201d (or perform the same function with the combination of buttons: \"Ctrl\" + \"I\" ). \u2014 In the window that appears, select the downloaded \".ova\" file from the Downloads folder and click \"Next\": \u2014 After click \"Import\" without any changes: \u2014 The operating system is importing: \u2014 Now your Virtual Machine with Linux operating system is ready to work, start it by clicking and use: Jupiter terminal start and Notebooks run errors in VM After starting your VM and running Notebooks, you may encounter the following error (look at the red frames in the photo below): To solve this error , follow these simple steps: \u2014 \u0421lose notebooks and terminal \u2014 Run terminal again and type this code ./run-server-jupyter.sh to start Jupiter \u2014 You have restarted your terminal and server successfully \u2014 Open notebook in your browser and restart kernel by clicking restart button Now your terminal and server should work. Notebook also runs without errors.","title":"VM with ROOT and Jupyter"},{"location":"vm/appendix/","text":"Ubuntu Create a virtual machine \u2014 Click the \"New\" button. \u2014 In the settings window specify the \"Name\" (in the future you can change), the \"Type\" and \"Version\" of the operating system and click \"Next\". For example - Name: My_Ubuntu (o_0), Type: Linux, Version: Ubuntu (64-bit). \u2014 The machine claims that the minimum memory size (RAM) requirements for full-fledged work Linux,, 1024MB. But if you want your machine to work well, then select the volume for more depending on the RAM of your computer (2 GB if RAM of your computer 8 GB) and click \"Next\". Sometimes even more, it all depends on the tasks and how many Virtual Machines you will run at the same time. \u2014 So we went to create a virtual hard disk. Click \"Create\": \u2014 Specify the type of hard disk (VDI - VirtualBox Disk Image) and click \"Next\": (Choose the first item, as it is easier. You have created a file that will simulate the hard disk of your virtual machine. If necessary, you can transfer it to another computer and run there). \u2014 In the window that appears, put a tick in \"Dynamically allocated\", after click \"Next\": -The hard disk size set at least 80 GB (or more) and click \"Create\". \u2014 Now the Virtual Machine is ready, but without an operating system. Start the Virtual Machine by clicking on the right mouse button and select \"Start\" -> \"Normal start\" . \u2014 When you first start you will be prompted to select the installation file from which the installation will be performed. You can download the installation file from the official website . Select the installation file from the Downloads folder (or where you saved it) and click \"Start\". \u2014 In the window that appears, select the operating system language and click on \"Install Ubuntu\". Next, select the keyboard language. \u2014 After selecting the keyboard language, just click on \"Continue\" \u2014 Disk layout . You can choose installation type. Simply format the entire hard drive and install Ubuntu on it, or select the manual option (Set the checkbox to \"Something else\" option and click \"Continue\"): \u2014 In the opened window, if you have not yet marked up the hard drive, you need to create a partition table, to do this, click \"New Partition Table...\". \u2014 Four sections are recommended for Linux: / - ext4, size 10-50 GB, for system installation /boot - ext2, size 100 MB, for bootloader files swap - swap, size equals RAM, for swap /home - ext4, all remaining space \u2014 To create a new section, click the \"+\" button: \u2014 Here you need to specify the mount point, for example, / or /home , size, file system and you can set a label: \u2014 In the end, you should have something like this: Then click \"Install Now\". \u2014 You should have this window: Check that everything is correct, click \"Continue\". \u2014 Select your time zone: \u2014 Enter your Username, computer name and password: \u2014 Installing your operating system: \u2014 Enjoy your virtual machine: VM with Docker container and Jupyter Lab","title":"Installing a generic Ubuntu VM"},{"location":"vm/appendix/#ubuntu","text":"Create a virtual machine \u2014 Click the \"New\" button. \u2014 In the settings window specify the \"Name\" (in the future you can change), the \"Type\" and \"Version\" of the operating system and click \"Next\". For example - Name: My_Ubuntu (o_0), Type: Linux, Version: Ubuntu (64-bit). \u2014 The machine claims that the minimum memory size (RAM) requirements for full-fledged work Linux,, 1024MB. But if you want your machine to work well, then select the volume for more depending on the RAM of your computer (2 GB if RAM of your computer 8 GB) and click \"Next\". Sometimes even more, it all depends on the tasks and how many Virtual Machines you will run at the same time. \u2014 So we went to create a virtual hard disk. Click \"Create\": \u2014 Specify the type of hard disk (VDI - VirtualBox Disk Image) and click \"Next\": (Choose the first item, as it is easier. You have created a file that will simulate the hard disk of your virtual machine. If necessary, you can transfer it to another computer and run there). \u2014 In the window that appears, put a tick in \"Dynamically allocated\", after click \"Next\": -The hard disk size set at least 80 GB (or more) and click \"Create\". \u2014 Now the Virtual Machine is ready, but without an operating system. Start the Virtual Machine by clicking on the right mouse button and select \"Start\" -> \"Normal start\" . \u2014 When you first start you will be prompted to select the installation file from which the installation will be performed. You can download the installation file from the official website . Select the installation file from the Downloads folder (or where you saved it) and click \"Start\". \u2014 In the window that appears, select the operating system language and click on \"Install Ubuntu\". Next, select the keyboard language. \u2014 After selecting the keyboard language, just click on \"Continue\" \u2014 Disk layout . You can choose installation type. Simply format the entire hard drive and install Ubuntu on it, or select the manual option (Set the checkbox to \"Something else\" option and click \"Continue\"): \u2014 In the opened window, if you have not yet marked up the hard drive, you need to create a partition table, to do this, click \"New Partition Table...\". \u2014 Four sections are recommended for Linux: / - ext4, size 10-50 GB, for system installation /boot - ext2, size 100 MB, for bootloader files swap - swap, size equals RAM, for swap /home - ext4, all remaining space \u2014 To create a new section, click the \"+\" button: \u2014 Here you need to specify the mount point, for example, / or /home , size, file system and you can set a label: \u2014 In the end, you should have something like this: Then click \"Install Now\". \u2014 You should have this window: Check that everything is correct, click \"Continue\". \u2014 Select your time zone: \u2014 Enter your Username, computer name and password: \u2014 Installing your operating system: \u2014 Enjoy your virtual machine:","title":"Ubuntu"},{"location":"vm/appendix/#vm-with-docker-container-and-jupyter-lab","text":"","title":"VM with Docker container and Jupyter Lab"}]}